{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf7bf83c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T07:09:51.920139Z",
     "start_time": "2023-10-23T07:09:51.900117Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import together\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "class TogetherLLM(LLM):\n",
    "    \"\"\"Together large language models.\"\"\"\n",
    "\n",
    "    model: str = \"togethercomputer/llama-2-13b-chat\"\n",
    "    \"\"\"model endpoint to use\"\"\"\n",
    "\n",
    "    together_api_key: str = os.environ['TOGETHERAI_API_KEY']\n",
    "    \"\"\"Together API key\"\"\"\n",
    "\n",
    "    temperature: float = 0.0\n",
    "    \"\"\"What sampling temperature to use.\"\"\"\n",
    "\n",
    "    max_tokens: int = 512\n",
    "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of LLM.\"\"\"\n",
    "        return \"together\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        **kwargs,\n",
    "    ) -> str:\n",
    "        \"\"\"Call to Together endpoint.\"\"\"\n",
    "        endpoint = 'https://api.together.xyz/inference'\n",
    "                \n",
    "        for attempt in range(10):\n",
    "            try:\n",
    "                res = requests.post(endpoint, json={\n",
    "                    \"prompt\": prompt,\n",
    "                    \"model\": self.model,\n",
    "                    \"temperature\": self.temperature,\n",
    "                    \"max_tokens\": self.max_tokens\n",
    "                }, headers={\n",
    "                    \"Authorization\": f\"Bearer {self.together_api_key}\",\n",
    "                    \"User-Agent\": \"<YOUR_APP_NAME>\"\n",
    "                })\n",
    "                output = res.json()['output']['choices'][0]['text']\n",
    "                return output\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        raise Exception(f\"Request did not succeed with prompt = {prompt}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d4a359e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T07:09:52.737485Z",
     "start_time": "2023-10-23T07:09:52.710138Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "class Utility:\n",
    "    B_CHAT, E_CHAT = \"<s>\", \"</s>\"\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_prompt(system_message, user_message, input_variables, history):\n",
    "        system_prompt = f\"{Utility.B_SYS}{system_message}{Utility.E_SYS}\"\n",
    "        \n",
    "        prompt_template_items = []\n",
    "                        \n",
    "        for index, (query, response) in enumerate(history):\n",
    "            if index == 0:\n",
    "                prompt_template_items.append(Utility.B_CHAT)\n",
    "                prompt_template_items.append(Utility.B_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(system_prompt)\n",
    "                prompt_template_items.append(query)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(Utility.E_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(response)\n",
    "                prompt_template_items.append(Utility.E_CHAT)\n",
    "            else:\n",
    "                prompt_template_items.append(Utility.B_CHAT)\n",
    "                prompt_template_items.append(Utility.B_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(query)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(Utility.E_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(response)\n",
    "                prompt_template_items.append(Utility.E_CHAT)\n",
    "        \n",
    "        if not history:\n",
    "            prompt_template_items.append(Utility.B_CHAT)\n",
    "            prompt_template_items.append(Utility.B_INST)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(system_prompt)\n",
    "            prompt_template_items.append(user_message)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(Utility.E_INST)\n",
    "        else:\n",
    "            prompt_template_items.append(Utility.B_CHAT)\n",
    "            prompt_template_items.append(Utility.B_INST)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(user_message)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(Utility.E_INST)\n",
    "        \n",
    "        prompt_template = \"\".join(prompt_template_items)\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=input_variables)\n",
    "        return prompt\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_type_from_response(response, search_term):\n",
    "        does_not_include_patterns = [\n",
    "            f\"\"\"that tweet as it is not {search_term.lower()}\"\"\",\n",
    "            f\"\"\"(the|this|the given) tweet.*is not {search_term.lower()}\"\"\",\n",
    "            f\"\"\"i.*classify (it|the tweet) as not {search_term.lower()}\"\"\",\n",
    "            f\"\"\"(?:the|this|the given)?\\s*(?:tweet|it) does not fall (into|under) the (?:category of )?{search_term.lower()}\"\"\",\n",
    "            f\"\"\"it is therefore classified as not {search_term.lower()}\"\"\",\n",
    "            f\"\"\"i would classify the (given )?tweet as not {search_term.lower()}\"\"\"\n",
    "        ]\n",
    "\n",
    "        includes_patterns = [\n",
    "            f\"\"\"(the|this|the given) tweet.*is {search_term.lower()}\"\"\",\n",
    "            f\"\"\"i.*classify (it|the tweet) as {search_term.lower()}\"\"\",\n",
    "            f\"\"\"(?:the|this|the given)?\\s*(?:tweet|it) (falls into|falls under|under) the (?:category of )?{search_term.lower()}\"\"\",\n",
    "            f\"\"\"this is a {search_term.lower()} claim as it is based on\"\"\",\n",
    "            f\"\"\"the tweet is classified as {search_term.lower()}\"\"\",\n",
    "            f\"\"\"this claim can be verified.*{search_term.lower()}\"\"\",\n",
    "            f\"\"\"therefore, it can be classified as {search_term.lower()}\"\"\",\n",
    "            f\"\"\"this is a direct statement about.*{search_term.lower()}\"\"\",\n",
    "            f\"\"\"so it falls under the category of {search_term.lower()}\"\"\",\n",
    "            f\"\"\"the tweet is reporting on a {search_term.lower()} fact\"\"\",\n",
    "            f\"\"\"this tweet contains a direct statement about.*{search_term.lower()}\"\"\"\n",
    "        ]\n",
    "\n",
    "        for pattern in does_not_include_patterns:\n",
    "            if re.search(pattern, response.lower()):\n",
    "                print(\"pattern does not include = \", pattern)\n",
    "                return '@'\n",
    "\n",
    "        for pattern in includes_patterns:\n",
    "            if re.search(pattern, response.lower()):\n",
    "                print(\"pattern include = \", pattern)\n",
    "                return '#'\n",
    "\n",
    "        for c in response[::-1]:\n",
    "            if c == '@' or c == '#':\n",
    "                print(\"Inside last search!\")\n",
    "                return c\n",
    "\n",
    "        raise Exception(f\"No @ or # found in response = {response}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_metrics(ground_truth, predicted):\n",
    "#         accuracy = accuracy_score(ground_truth, predicted)\n",
    "#         precision = precision_score(ground_truth, predicted)\n",
    "#         recall = recall_score(ground_truth, predicted)\n",
    "#         f1 = f1_score(ground_truth, predicted)\n",
    "        \n",
    "        clsf_report = classification_report(y_true = ground_truth, y_pred = predicted, output_dict=True)\n",
    "        cf_matrix = confusion_matrix(ground_truth, predicted)\n",
    "        \n",
    "        precision = clsf_report['weighted avg']['precision']\n",
    "        recall = clsf_report['weighted avg']['recall']\n",
    "        f1 = clsf_report['weighted avg']['f1-score']\n",
    "        accuracy = accuracy_score(ground_truth, predicted)\n",
    "        \n",
    "        return {\n",
    "            \"Accuracy\": accuracy * 100,\n",
    "            \"Precision\": precision * 100,\n",
    "            \"Recall\": recall * 100,\n",
    "            \"F1\": f1 * 100,\n",
    "            \"Confusion Matrix\": cf_matrix\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tweet_data(file_name):\n",
    "        df = pd.read_csv(file_name, index_col=0)\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_prediction_output(tweet_objects, file_name_to_write):\n",
    "        if os.path.exists(file_name_to_write):\n",
    "            os.remove(file_name_to_write)\n",
    "        \n",
    "        tweet_objects.to_csv(file_name_to_write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbd491e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T07:09:53.301089Z",
     "start_time": "2023-10-23T07:09:53.289226Z"
    }
   },
   "outputs": [],
   "source": [
    "class Default(dict):\n",
    "    def __missing__(self, key):\n",
    "        return f\"{{{key}}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f39ff3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T07:10:07.522546Z",
     "start_time": "2023-10-23T07:10:07.498565Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "class Category:\n",
    "    INPUT_VARIABLES=[\"delimiter\", \"tweet\"]\n",
    "    \n",
    "    CATEGORY_DESCRIPTIONS = {1: \"Scientifically Verifiable\"}\n",
    "    \n",
    "    DELIMITER = \"```\"\n",
    "    \n",
    "    def __init__(self, category_type, llm):\n",
    "        self.category_type = category_type\n",
    "        \n",
    "        self.llm = llm\n",
    "        self.history = []\n",
    "        \n",
    "    def does_tweet_fall_into_category(self, tweet, was_previous_classification_correct=None):\n",
    "        print(\"Inside does_tweet_fall_into_category function\")\n",
    "                \n",
    "        if was_previous_classification_correct is None:\n",
    "            user_message = \"\"\"\n",
    "             Classify the following tweet:\n",
    "             Tweet: {delimiter} {tweet} {delimiter}\n",
    "             \"\"\"\n",
    "        else:\n",
    "            if was_previous_classification_correct:\n",
    "                user_message = \"\"\"\n",
    "                 Your previous classification was correct. Now, classify the following tweet:\n",
    "                 Tweet: {delimiter} {tweet} {delimiter}\n",
    "                 \"\"\"\n",
    "            else:\n",
    "                user_message = \"\"\"\n",
    "                 Your previous classification was incorrect. Now, classify the following tweet:\n",
    "                 Tweet: {delimiter} {tweet} {delimiter}\n",
    "                 \"\"\"\n",
    "        \n",
    "        verifiable_claim_chain_prompt = Utility.get_prompt(self.verifiable_claim_chain_system_message, \n",
    "                                                           user_message, Category.INPUT_VARIABLES, [])\n",
    "        \n",
    "        non_verifiable_claim_chain_prompt = Utility.get_prompt(self.non_verifiable_claim_chain_system_message, \n",
    "                                                               user_message, Category.INPUT_VARIABLES, [])\n",
    "\n",
    "        verifiable_claim_chain = LLMChain(llm = self.llm, \n",
    "                                          prompt = verifiable_claim_chain_prompt)\n",
    "        \n",
    "        non_verifiable_claim_chain = LLMChain(llm = self.llm, \n",
    "                                          prompt = non_verifiable_claim_chain_prompt)\n",
    "\n",
    "        input_values = {\"tweet\": tweet, \"delimiter\": Category.DELIMITER}\n",
    "        \n",
    "        verifiable_claim_chain_response = verifiable_claim_chain.run(input_values)\n",
    "        non_verifiable_claim_chain_response = non_verifiable_claim_chain.run(input_values)\n",
    "        \n",
    "        verifiable_claim_chain_response_number = Utility.extract_number(verifiable_claim_chain_response)\n",
    "        non_verifiable_claim_chain_response_number = Utility.extract_number(non_verifiable_claim_chain_response)\n",
    "        \n",
    "        if verifiable_claim_chain_response_number != non_verifiable_claim_chain_response_number:\n",
    "            return verifiable_claim_chain_response_number\n",
    "        \n",
    "        arbitrer_claim_chain_user_message = \"\"\"\n",
    "        Response1: {response1}\n",
    "        Response2: {response2}\n",
    "        \"\"\"\n",
    "        \n",
    "        arbitrer_claim_chain_input_variables = [\"response1\", \"response2\"]\n",
    "        \n",
    "        arbitrer_claim_chain_prompt = Utility.get_prompt(self.arbitrer_claim_chain_system_message, \n",
    "                                                         arbitrer_claim_chain_user_message, \n",
    "                                                         arbitrer_claim_chain_input_variables,\n",
    "                                                         self.history)\n",
    "        \n",
    "        arbitrer_claim_input_values = {\"response1\": verifiable_claim_chain_response, \n",
    "                                       \"response2\": non_verifiable_claim_chain_response}\n",
    "        \n",
    "                \n",
    "        arbitrer_claim_chain = LLMChain(llm = self.llm, \n",
    "                                        prompt = arbitrer_claim_chain_prompt)\n",
    "        \n",
    "        arbitrer_response = arbitrer_claim_chain.run(arbitrer_claim_input_values)\n",
    "        \n",
    "        return Utility.extract_number(arbitrer_response)\n",
    "        \n",
    "    def generate_cat_metrics(self, output_file_name, tweet_content_column=\"polished_text\"):\n",
    "        ground_truths = []\n",
    "        predicted_outputs = []\n",
    "\n",
    "        print(\"<======= Generating metrics for category type =\", self.category_type, \"=======>\")\n",
    "        print()\n",
    "\n",
    "        # check if predicted_cat_type column exists. If not, create it.\n",
    "\n",
    "        category_type_prediction_column_name = f\"predicted_{self.category_type}\"\n",
    "        \n",
    "        start_index = 0\n",
    "        end_index = 200\n",
    "\n",
    "        if category_type_prediction_column_name not in self.tweet_objects:\n",
    "            self.tweet_objects[category_type_prediction_column_name] = -1\n",
    "        else:\n",
    "            for index in range(start_index, end_index + 1):\n",
    "                if index not in self.tweet_objects[column_name]:\n",
    "                    continue\n",
    "\n",
    "                if self.tweet_objects[category_type_prediction_column_name][index] != -1:\n",
    "                    raise Exception(\n",
    "                        \"Some of the indices for the specified range have already been computed.\"\n",
    "                    )\n",
    "        \n",
    "        was_previous_classification_correct = None\n",
    "        \n",
    "        for index in range(start_index, end_index + 1):\n",
    "            if index not in self.tweet_objects[tweet_content_column]:\n",
    "                continue\n",
    "                \n",
    "            print(\"Processing tweet with index# =\", index)\n",
    "            tweet = self.tweet_objects.iloc[index][tweet_content_column]\n",
    "            print(\"Tweet content = \", tweet)\n",
    "            \n",
    "            for attempt in range(10):\n",
    "                try:\n",
    "                    ground_truth = int(self.tweet_objects.iloc[index][f\"cat{self.category_type}\"])\n",
    "                except:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "                                \n",
    "            for attempt in range(10):\n",
    "                try:\n",
    "                    predicted_output = self.does_tweet_fall_into_category(tweet,\n",
    "                                                                          was_previous_classification_correct)\n",
    "                    \n",
    "                    if predicted_output is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if predicted_output is None:\n",
    "                print(\"None for index# = \", index, \"and tweet content =\", tweet)\n",
    "                raise Exception(\"Did not get predicted output for tweet\")\n",
    "                \n",
    "            if index > 0 and index % 5 == 0:\n",
    "                print(\"Metrics till now =\", Utility.calculate_metrics(ground_truths, predicted_outputs))\n",
    "\n",
    "            ground_truths.append(ground_truth)\n",
    "            predicted_outputs.append(predicted_output)\n",
    "            \n",
    "            response_list = [\"Tweet is not scientifically verifiable\", \"Tweet is scientifically verifiable\"]\n",
    "            \n",
    "#             if len(self.history) == 20:\n",
    "#                 self.history.pop(0)\n",
    "            \n",
    "#             self.history.append((f\"Tweet = {tweet}\", response_list[predicted_output]))\n",
    "            \n",
    "            if ground_truth == predicted_output:\n",
    "                was_previous_classification_correct = True\n",
    "            else:\n",
    "                was_previous_classification_correct = False\n",
    "            \n",
    "            self.tweet_objects.loc[index, category_type_prediction_column_name] = predicted_output\n",
    "\n",
    "            print(\"Ground truth =\", ground_truth, \"Predicted output =\", predicted_output)\n",
    "            print(\"Finished Processing tweet with index# =\", index)\n",
    "            print()\n",
    "\n",
    "        print(\"<======= Finished generating metrics for claim existence =======>\")\n",
    "        \n",
    "        print(\"Ground truths = \", ground_truths)\n",
    "        print(\"Predictions = \", predicted_outputs)\n",
    "        \n",
    "        Utility.write_prediction_output(self.tweet_objects, output_file_name)\n",
    "        \n",
    "        return Utility.calculate_metrics(ground_truths, predicted_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13b2c52f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T07:10:07.991800Z",
     "start_time": "2023-10-23T07:10:07.967901Z"
    }
   },
   "outputs": [],
   "source": [
    "class Category1(Category):\n",
    "    CATEGORY_TYPE = 1\n",
    "    CATEGORY_DESCRIPTION = \"\"\n",
    "    \n",
    "    def __init__(self, llm, input_file_name):\n",
    "        self.verifiable_claim_chain_system_message, \\\n",
    "        self.non_verifiable_claim_chain_system_message, \\\n",
    "        self.arbitrer_claim_chain_system_message, \\\n",
    "        self.tweet_objects = Category1.generate_system_prompt_for_category1(input_file_name)\n",
    "        \n",
    "        super().__init__(Category1.CATEGORY_TYPE, llm)\n",
    "        \n",
    "    @staticmethod\n",
    "    def generate_system_prompt_for_category1(input_file_name):\n",
    "        category1_indices = [22, 56, 71]\n",
    "        non_category_1_indices = [21, 61, 90]\n",
    "\n",
    "        tweet_examples_of_category1 = \"\"\"\n",
    "        Some examples of tweets that ARE scientifically verifiable (expected response 1):\n",
    "            a) \" ::people_holding_hands:: We can now meet our family and friends outdoors in a group of 6, or 2 households ::leftright_arrow:: Its important that when we do, we follow social distancing guidance ::backhand_index_pointing_right:: This will help to stop the spread of COVID19 as we take the next step out of lockdown LetsDoItForLancashire \"\n",
    "            b) \": BREAKING: Dozens of cops in Massachusetts have resigned in protest of the vaccine mandates. TO WISH THEM GOOD RIDDA\"\n",
    "            c) \": BREAKING Syria president and first lady test positive for COVID19: presidency AFP\"\n",
    "        \"\"\"\n",
    "        \n",
    "        tweet_examples_of_non_category1 = \"\"\"\n",
    "        Some examples of tweets ARE NOT scientifically verifiable (expected response 0):\n",
    "            a) \" : The ones calling for lockdown, without risk or injury to themselves, should pay up.\"\n",
    "            b) \": Can you catch coronavirus from handling cash? A new study says the risk is low\"\n",
    "            c) \": I wouldnt trust anything this man touches. NoVaccineForMe\"\n",
    "        \"\"\"\n",
    "        \n",
    "        verifiable_claim_chain_system_message = \"\"\"\n",
    "        Imagine you're a COVID-19 tweets classifier. You will determine whether tweets fall into scientifically verifiable claim category using the following guidelines:\n",
    "\n",
    "            I) Direct statements about the COVID-19 virus, its origin, its transmission, prevention methods, or symptoms etc ARE scientifically verifiable. For example:\n",
    "                - Example 1: \"Masks don't work against COVID-19.\"\n",
    "                - Example 2: \"The government needs to get to the bottom of COVID-19 origin and Chinese involvement.\"\n",
    "\n",
    "            II) Opinionated, anecdotal, or hearsay claims about COVID-19 topics MAY BE scientifically verifiable:\n",
    "                - Example 1: \"Talked to a friend who believes the virus started from bats in a wet market. Sounds plausible.\" (Hearsay)\n",
    "                - Example 2: \"Got my vaccine yesterday and I feel great! Proof that it works\" (Anecdote)\n",
    "                - Example 3: \"Based on my research, I'm convinced the virus started from bats in a wet market.\" (Opinion)\n",
    "                - Example 4: \"Don't forget to practice social distancing. It will keep everyone safe.\" (Opinion)\n",
    "\n",
    "            III) Reports on cases, deaths, or someone testing positive ARE scientifically verifiable. For example:\n",
    "                - Example 1: \"Justin Bieber has tested positive for COVID19\"\n",
    "                - Example 3: \"Almost 2000 people have died from COVID-19 in Brazil\"\n",
    "\n",
    "        For a given tweet, step through each of the points. If there's a match with one of the points, return 1. If there's no match with any of the points, return 0. Your response should contain the entire process of stepping through the guidelines.\n",
    "\n",
    "        Output Instructions:\n",
    "                - Tweet = \"Yall can try that damn vaccine on yourselves first! Im not trying to turn into an anamorph\"\n",
    "                    - point I) doesn't match as this is not a direct statement about COVID-19 topics.\n",
    "                    - point II) matches as the tweet contains an opinion about the vaccine (a COVID-19 topic)\n",
    "                        - The tweet is scientifically verifiable. Return 1.\n",
    "\n",
    "                - Tweet = \"Yo jus dont use Amazon this lockdown, well were all gettin burnt 2 the ground, bezos just rackin n not payin tax to help the nhs, schools, carework, nothing.. if u feed the beast yr just helpin them destroy us..\"\n",
    "                    - point I) doesn't match as there's no direct statement about COVID-19 topics\n",
    "                    - point II) doesn't match as there's no opinions about COVID-19 topics\n",
    "                    - point III) doesn't match as there's no report\n",
    "                    - The tweet is not scientifically verifiable. Return 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        non_verifiable_claim_chain_system_message = \"\"\"\n",
    "        Imagine you're a COVID-19 tweets classifier. You will determine whether tweets do not fall into scientifically verifiable claim category using the following guidelines:\n",
    "\n",
    "            I) Observational statements ARE NOT scientifically verifiable. For example:\n",
    "                - Example 1: \"Many people aren't wearing masks\"\n",
    "                - Example 2: \"People aren't really following social distancing, apparently\"\n",
    "\n",
    "            II) Impacts of COVID-19 on fields other than science - Business, Law, Histoy, Politics, Operations etc ARE NOT scientifically verifiable. For example:\n",
    "                - Example 1: \"We haven't been able to open our restaurant as COVID-19 has impacted us to operate at full capacity.\" (COVID-19's effect on Business)\n",
    "                - Example 2: \"COVID-19 disrupted global supply chains, leading to shortages of essential goods and a rise in production costs.\" (COVID-19's effect on Operations)\n",
    "                - Example 3: \"Legal disputes over lease agreements surged during the pandemic, particularly where tenants were unable to meet their rental obligations due to lockdowns\" (COVID-19's effect on Law)\n",
    "                - Example 4: \"The pandemic triggered geopolitical tensions, with countries competing for access to limited vaccine supplies and engaging in 'vaccine diplomacy'.\" (COVID-19's effect on Politics)\n",
    "                - Example 5: \"Film and television production faced long hiatuses, and when resumed, had to adapt to strict health protocols.\" (COVID-19's effect on Entertainment)\n",
    "\n",
    "            III) Second-hand opinions or queries ARE NOT scientifically verifiable. For example:\n",
    "                - Example 1: \"My neighbor says that social distancing is just a way to keep us apart and isolated. Thoughts?\" (The opinion is not author's, but his neighbor's)\n",
    "                - Example 2: \"They have said we need more social distancing even after vaccines. I don't understand why.\" (The first part is not author's saying.)\n",
    "\n",
    "            IV) Asking questions without a direct claim ARE NOT scientifically verifiable. For example:\n",
    "                - Example 1: \"Why do we still need to wear masks after vaccination?\"\n",
    "                - Example 2: \"I'm surprised. Weren't they all vaccinated at the company conference?\"\n",
    "\n",
    "            V) Instructions, Information, notifications or announcements that do not contain opinions about COVID-19 topics ARE NOT scientifically verifiable. For examples:\n",
    "                - Example 1: \"You need to wear masks and follow social distancing to get on buses, trains or planes\" (Instructions)\n",
    "                - Example 2: \"Due to social distancing, our restaurant hasn't been able to operate at full capacity\" (Dispatching Information)\n",
    "                - Example 3: \"Travel advisory: If you're returning from a hotspot, you need to self-quarantine for 14 days.\" (Announcement)\n",
    "                - Example 4: \"You will receive communication if you are eligible for the vaccine.\" (Notification)\n",
    "                - Example 5: \"Get your free COVID-19 test by just walking in a clinic today\" (Announcement)\n",
    "                - Example 6: \"A hospital is using a new software to track COVID-19 cases.\"\n",
    "\n",
    "            VI) Political, Business or Legal motive behind COVID-19 topics ARE NOT scientifically verifiable. For example:\n",
    "                - Example 1: \"The Trump administration could have sped up the vaccine development process. If it was a democratic president, they would have dont it.\"\n",
    "                - Example 2: \"The pharmaceutical companies were not pressured politically by the government to deliver vaccines.\"\n",
    "\n",
    "            VII) Phrases like \"Read the whole story here\", \"Full version\", \"This story from\", \"Live Video\", \"How it became\", \"Here's a quick look\" etc. means it is a news reporting. These tweets DO NOT contain scientifically verifiable claim.\n",
    "\n",
    "        For a given tweet, step through each of the points. If there's a match with one of the points, return 1. If there's no match with any of the points, return 0. Your response should contain the entire process of stepping through the guidelines.\n",
    "\n",
    "        Output Instructions:\n",
    "        - Tweet = \"Yall can try that damn vaccine on yourselves first! Im not trying to turn into an anamorph\"\n",
    "            - point I) doesn't match as this is not an observation.\n",
    "            - point II) doesn't match as the tweet doesn't mention impact of non-scientific fields\n",
    "            - point III) doesn't match as the tweet doesn't contain second-hand opinions\n",
    "            - point IV) doesn't match as the tweet doesn't pose a question\n",
    "            - point V) doesn't match as the tweet doesn't contain instructions, info, notification or annoucements that do not contain opinion\n",
    "            - point VI) doesn't match as there's no motive\n",
    "            - point VII) doesn't match as there's no such phrase\n",
    "            - The tweet is scientifically verifiable. Return 0.\n",
    "\n",
    "        - Tweet = \"Yo jus dont use Amazon this lockdown, well were all gettin burnt 2 the ground, bezos just rackin n not payin tax to help the nhs, schools, carework, nothing.. if u feed the beast yr just helpin them destroy us..\"\n",
    "            - point I) doesn't match as this is not an observation.\n",
    "            - point II) matches as the tweet mentions the impact of Amazon's predatory business policies on everyday life.\n",
    "            - The tweet is not scientifically verifiable. Return 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        arbitrer_claim_chain_system_message = \"\"\"\n",
    "        Imagine you're a COVID-19 tweets classifier. You will receive two responses for each tweet - one being the argument that the tweet is scientifically verifiable, and other that the tweet is not scientifically verifiable. Your job is to decide which one is correct.\n",
    "\n",
    "        Give your reasoning which one of the two responses is more accurate using the following guidelines:\n",
    "            I) Direct statements about the COVID-19 virus, its origin, its transmission, prevention methods, or symptoms etc ARE scientifically verifiable.\n",
    "            II) Opinionated, anecdotal, or hearsay claims about COVID-19 topics MAY BE scientifically verifiable.\n",
    "            III) Reports on cases, deaths, or someone testing positive ARE scientifically verifiable.\n",
    "            IV) Observational statements ARE NOT scientifically verifiable. Focus on the difference between observations and opinions. Opinions have claims. Observations do not have claims.\n",
    "            V) Impacts of COVID-19 on fields other than science - Business, Law, Histoy, Politics, Operations etc ARE NOT scientifically verifiable.\n",
    "            VI) Second-hand opinions or queries ARE NOT scientifically verifiable.\n",
    "            VII) Asking questions without a direct claim ARE NOT scientifically verifiable.\n",
    "            VIII) Instructions, Information, notifications or announcements that do not contain opinions about COVID-19 topics ARE NOT scientifically verifiable.\n",
    "            IX) Political, Business or Legal motive behind COVID-19 topics ARE NOT scientifically verifiable.\n",
    "            X) Phrases like \"Read the whole story here\", \"Full version\", \"This story from\", \"Live Video\", \"How it became\", \"Here's a quick look\" etc. means it is a news reporting. These tweets DO NOT contain scientifically verifiable claim.\n",
    "            \n",
    "        {tweet_examples_of_category1}\n",
    "        \n",
    "        {tweet_examples_of_non_category1}\n",
    "\n",
    "        If the tweet is scientifically verifiable, return 1. Otherwise, return 0.\n",
    "        \"\"\".format_map(Default(tweet_examples_of_non_category1=tweet_examples_of_non_category1, \\\n",
    "                               tweet_examples_of_category1=tweet_examples_of_category1))\n",
    "        \n",
    "        tweet_objects = Utility.get_tweet_data(input_file_name)\n",
    "        indices_to_ignore = category1_indices + non_category_1_indices\n",
    "        filtered_tweet_objects = tweet_objects.drop(indices_to_ignore)\n",
    "        \n",
    "        return verifiable_claim_chain_system_message, \\\n",
    "                non_verifiable_claim_chain_system_message, \\\n",
    "                arbitrer_claim_chain_system_message, \\\n",
    "                filtered_tweet_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0bb48c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T07:10:08.373563Z",
     "start_time": "2023-10-23T07:10:08.362951Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "llm = TogetherLLM(\n",
    "    model= \"togethercomputer/llama-2-70b-chat\",\n",
    "    temperature=0.15,\n",
    "    max_tokens=1500\n",
    ")\n",
    "\n",
    "input_file_name = \"tweets - original.csv\"\n",
    "output_file_name = \"updated_tweets_cat_1-70b.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bb0c7d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-23T07:10:08.970737Z",
     "start_time": "2023-10-23T07:10:08.897470Z"
    }
   },
   "outputs": [],
   "source": [
    "cat1 = Category1(llm, input_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
