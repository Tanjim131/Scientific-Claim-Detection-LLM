{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf7bf83c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:42:52.381738Z",
     "start_time": "2023-11-06T17:42:51.013834Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import together\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "class TogetherLLM(LLM):\n",
    "    \"\"\"Together large language models.\"\"\"\n",
    "\n",
    "    model: str = \"togethercomputer/llama-2-13b-chat\"\n",
    "    \"\"\"model endpoint to use\"\"\"\n",
    "\n",
    "    together_api_key: str = os.environ['TOGETHERAI_API_KEY']\n",
    "    \"\"\"Together API key\"\"\"\n",
    "\n",
    "    temperature: float = 0.0\n",
    "    \"\"\"What sampling temperature to use.\"\"\"\n",
    "\n",
    "    max_tokens: int = 512\n",
    "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of LLM.\"\"\"\n",
    "        return \"together\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        **kwargs,\n",
    "    ) -> str:\n",
    "        \"\"\"Call to Together endpoint.\"\"\"\n",
    "        endpoint = 'https://api.together.xyz/inference'\n",
    "        \n",
    "        print(\"model =\", self.model)\n",
    "        print(\"temperature =\", self.temperature)\n",
    "        print(\"max_tokens=\", self.max_tokens)\n",
    "                \n",
    "        for attempt in range(10):\n",
    "            try:\n",
    "                res = requests.post(endpoint, json={\n",
    "                    \"prompt\": prompt,\n",
    "                    \"model\": self.model,\n",
    "                    \"temperature\": self.temperature,\n",
    "                    \"max_tokens\": self.max_tokens\n",
    "                }, headers={\n",
    "                    \"Authorization\": f\"Bearer {self.together_api_key}\",\n",
    "                    \"User-Agent\": \"<YOUR_APP_NAME>\"\n",
    "                })\n",
    "                output = res.json()['output']['choices'][0]['text']\n",
    "                return output\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        raise Exception(f\"Request did not succeed with prompt = {prompt}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d4a359e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:42:53.101370Z",
     "start_time": "2023-11-06T17:42:52.381738Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "class Utility:\n",
    "    B_CHAT, E_CHAT = \"<s>\", \"</s>\"\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_prompt(system_message, user_message, input_variables, history):\n",
    "        system_prompt = f\"{Utility.B_SYS}{system_message}{Utility.E_SYS}\"\n",
    "        \n",
    "        prompt_template_items = []\n",
    "                        \n",
    "        for index, (query, response) in enumerate(history):\n",
    "            if index == 0:\n",
    "                prompt_template_items.append(Utility.B_CHAT)\n",
    "                prompt_template_items.append(Utility.B_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(system_prompt)\n",
    "                prompt_template_items.append(query)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(Utility.E_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(response)\n",
    "                prompt_template_items.append(Utility.E_CHAT)\n",
    "            else:\n",
    "                prompt_template_items.append(Utility.B_CHAT)\n",
    "                prompt_template_items.append(Utility.B_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(query)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(Utility.E_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(response)\n",
    "                prompt_template_items.append(Utility.E_CHAT)\n",
    "        \n",
    "        if not history:\n",
    "            prompt_template_items.append(Utility.B_CHAT)\n",
    "            prompt_template_items.append(Utility.B_INST)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(system_prompt)\n",
    "            prompt_template_items.append(user_message)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(Utility.E_INST)\n",
    "        else:\n",
    "            prompt_template_items.append(Utility.B_CHAT)\n",
    "            prompt_template_items.append(Utility.B_INST)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(user_message)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(Utility.E_INST)\n",
    "        \n",
    "        prompt_template = \"\".join(prompt_template_items)\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=input_variables)\n",
    "        return prompt\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_number(s):        \n",
    "#         start_index = s.find('{')\n",
    "#         end_index = s.find('}') + 1\n",
    "        \n",
    "#         substring = s[start_index : end_index]\n",
    "#         data = json.loads(substring)\n",
    "        \n",
    "#         key = list(data.keys())[0]\n",
    "#         value = data[key]\n",
    "        \n",
    "#         print(\"value = \", value)\n",
    "                \n",
    "#         if value == 0 or value == 1:\n",
    "#             return value\n",
    "        \n",
    "        for c in s[::-1]:\n",
    "            if c == '0' or c == '1':\n",
    "                return int(c)\n",
    "\n",
    "        raise Exception(f\"No 0 or 1 found in response = {s}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_metrics(ground_truth, predicted):\n",
    "#         accuracy = accuracy_score(ground_truth, predicted)\n",
    "#         precision = precision_score(ground_truth, predicted)\n",
    "#         recall = recall_score(ground_truth, predicted)\n",
    "#         f1 = f1_score(ground_truth, predicted)\n",
    "        \n",
    "        clsf_report = classification_report(y_true = ground_truth, y_pred = predicted, output_dict=True)\n",
    "        cf_matrix = confusion_matrix(ground_truth, predicted)\n",
    "        \n",
    "        precision = clsf_report['weighted avg']['precision']\n",
    "        recall = clsf_report['weighted avg']['recall']\n",
    "        f1 = clsf_report['weighted avg']['f1-score']\n",
    "        accuracy = accuracy_score(ground_truth, predicted)\n",
    "        \n",
    "        return {\n",
    "            \"Accuracy\": accuracy * 100,\n",
    "            \"Precision\": precision * 100,\n",
    "            \"Recall\": recall * 100,\n",
    "            \"F1\": f1 * 100,\n",
    "            \"Confusion Matrix\": cf_matrix\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tweet_data(file_name):\n",
    "        df = pd.read_csv(file_name, index_col=0)\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_prediction_output(tweet_objects, file_name_to_write):\n",
    "        if os.path.exists(file_name_to_write):\n",
    "            os.remove(file_name_to_write)\n",
    "        \n",
    "        tweet_objects.to_csv(file_name_to_write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd491e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:42:53.107101Z",
     "start_time": "2023-11-06T17:42:53.101370Z"
    }
   },
   "outputs": [],
   "source": [
    "class Default(dict):\n",
    "    def __missing__(self, key):\n",
    "        return f\"{{{key}}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f39ff3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:42:53.123043Z",
     "start_time": "2023-11-06T17:42:53.108757Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "class Category:\n",
    "    INPUT_VARIABLES=[\"delimiter\", \"tweet\"]\n",
    "    \n",
    "    DELIMITER = \"```\"\n",
    "    \n",
    "    def __init__(self, category_type, llm):\n",
    "        self.category_type = category_type\n",
    "        \n",
    "        self.llm = llm\n",
    "        self.history = []\n",
    "        \n",
    "    def does_tweet_fall_into_category(self, tweet):\n",
    "        print(\"Inside does_tweet_fall_into_category function\")\n",
    "                \n",
    "        \n",
    "        user_message = \"\"\"\n",
    "         Classify the following tweet:\n",
    "         Tweet: {delimiter} {tweet} {delimiter}\n",
    "         \"\"\"\n",
    "        \n",
    "        prompt = Utility.get_prompt(self.system_message, user_message, Category.INPUT_VARIABLES, [])\n",
    "\n",
    "        chain = LLMChain(llm = self.llm, prompt = prompt)\n",
    "\n",
    "        input_values = {\"tweet\": tweet, \"delimiter\": Category.DELIMITER}\n",
    "        \n",
    "        response = chain.run(input_values)\n",
    "        \n",
    "        print(\"Response =\", response)\n",
    "        \n",
    "        response_number = Utility.extract_number(response)\n",
    "        \n",
    "        return response_number\n",
    "        \n",
    "    def generate_cat_metrics(self, output_file_name, tweet_content_column=\"polished_text\"):\n",
    "        ground_truths = []\n",
    "        predicted_outputs = []\n",
    "\n",
    "        print(\"<======= Generating metrics for category type =\", self.category_type, \"=======>\")\n",
    "        print()\n",
    "\n",
    "        # check if predicted_cat_type column exists. If not, create it.\n",
    "\n",
    "        category_type_prediction_column_name = f\"predicted_{self.category_type}\"\n",
    "        \n",
    "        start_index = 0\n",
    "        end_index = 400\n",
    "\n",
    "        if category_type_prediction_column_name not in self.tweet_objects:\n",
    "            self.tweet_objects[category_type_prediction_column_name] = -1\n",
    "        else:\n",
    "            for index in range(start_index, end_index + 1):\n",
    "                if index not in self.tweet_objects[column_name]:\n",
    "                    continue\n",
    "\n",
    "                if self.tweet_objects[category_type_prediction_column_name][index] != -1:\n",
    "                    raise Exception(\n",
    "                        \"Some of the indices for the specified range have already been computed.\"\n",
    "                    )\n",
    "        \n",
    "#         was_previous_classification_correct = None\n",
    "        \n",
    "        for index in range(start_index, end_index + 1):\n",
    "            if index not in self.tweet_objects[tweet_content_column]:\n",
    "                continue\n",
    "                \n",
    "            print(\"Processing tweet with index# =\", index)\n",
    "            tweet = self.tweet_objects.iloc[index][tweet_content_column]\n",
    "            print(\"Tweet content = \", tweet)\n",
    "            \n",
    "            for attempt in range(10):\n",
    "                try:\n",
    "                    ground_truth = int(self.tweet_objects.iloc[index][f\"cat{self.category_type}\"])\n",
    "                except:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "                                \n",
    "            for attempt in range(10):\n",
    "                try:\n",
    "                    predicted_output = self.does_tweet_fall_into_category(tweet)\n",
    "                    \n",
    "                    if predicted_output is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if predicted_output is None:\n",
    "                print(\"None for index# = \", index, \"and tweet content =\", tweet)\n",
    "                raise Exception(\"Did not get predicted output for tweet\")\n",
    "                \n",
    "            if index > 0 and index % 5 == 0:\n",
    "                print(\"Metrics till now =\", Utility.calculate_metrics(ground_truths, predicted_outputs))\n",
    "\n",
    "            ground_truths.append(ground_truth)\n",
    "            predicted_outputs.append(predicted_output)\n",
    "            \n",
    "            response_list = [\"Tweet is not scientifically verifiable\", \"Tweet is scientifically verifiable\"]\n",
    "            \n",
    "#             if len(self.history) == 20:\n",
    "#                 self.history.pop(0)\n",
    "            \n",
    "#             self.history.append((f\"Tweet = {tweet}\", response_list[predicted_output]))\n",
    "            \n",
    "#             if ground_truth == predicted_output:\n",
    "#                 was_previous_classification_correct = True\n",
    "#             else:\n",
    "#                 was_previous_classification_correct = False\n",
    "            \n",
    "            self.tweet_objects.loc[index, category_type_prediction_column_name] = predicted_output\n",
    "\n",
    "            print(\"Ground truth =\", ground_truth, \"Predicted output =\", predicted_output)\n",
    "            print(\"Finished Processing tweet with index# =\", index)\n",
    "            print()\n",
    "\n",
    "        print(\"<======= Finished generating metrics for claim existence =======>\")\n",
    "        \n",
    "        print(\"Ground truths = \", ground_truths)\n",
    "        print(\"Predictions = \", predicted_outputs)\n",
    "        \n",
    "        Utility.write_prediction_output(self.tweet_objects, output_file_name)\n",
    "        \n",
    "        return Utility.calculate_metrics(ground_truths, predicted_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13b2c52f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:42:53.139080Z",
     "start_time": "2023-11-06T17:42:53.123705Z"
    }
   },
   "outputs": [],
   "source": [
    "class Category1(Category):\n",
    "    CATEGORY_TYPE = 1\n",
    "    CATEGORY_DESCRIPTION = \"\"\n",
    "    \n",
    "    def __init__(self, llm, input_file_name):\n",
    "        self.system_message, self.tweet_objects = Category1.generate_system_prompt_for_category1(input_file_name)\n",
    "        \n",
    "        super().__init__(Category1.CATEGORY_TYPE, llm)\n",
    "        \n",
    "    @staticmethod\n",
    "    def generate_system_prompt_for_category1(input_file_name):\n",
    "        category1_indices = [22, 56, 71]\n",
    "        non_category_1_indices = [21, 61, 90]\n",
    "\n",
    "        tweet_examples_of_category1 = \"\"\"\n",
    "        Some examples of tweets that ARE scientifically verifiable (expected response 1):\n",
    "            a) \" ::people_holding_hands:: We can now meet our family and friends outdoors in a group of 6, or 2 households ::leftright_arrow:: Its important that when we do, we follow social distancing guidance ::backhand_index_pointing_right:: This will help to stop the spread of COVID19 as we take the next step out of lockdown LetsDoItForLancashire \"\n",
    "            b) \": BREAKING: Dozens of cops in Massachusetts have resigned in protest of the vaccine mandates. TO WISH THEM GOOD RIDDA\"\n",
    "            c) \": BREAKING Syria president and first lady test positive for COVID19: presidency AFP\"\n",
    "        \"\"\"\n",
    "        \n",
    "        tweet_examples_of_non_category1 = \"\"\"\n",
    "        Some examples of tweets ARE NOT scientifically verifiable (expected response 0):\n",
    "            a) \" : The ones calling for lockdown, without risk or injury to themselves, should pay up.\"\n",
    "            b) \": Can you catch coronavirus from handling cash? A new study says the risk is low\"\n",
    "            c) \": I wouldnt trust anything this man touches. NoVaccineForMe\"\n",
    "        \"\"\"\n",
    "        \n",
    "        system_message = \"\"\"\n",
    "        Imagine you're a COVID-19 tweets classifier. You need to determine whether tweets fall into scientifically verifiable claim category.\n",
    "        \n",
    "        The tweets will be delimited with {delimiter} characters.\n",
    "\n",
    "        A claim or a question is scientifically verified if it's scientifically shown to be true or scientifically shown to be false.\n",
    "            \n",
    "        {tweet_examples_of_category1}\n",
    "        \n",
    "        {tweet_examples_of_non_category1}\n",
    "\n",
    "        If the tweet is scientifically verifiable, return 1. Otherwise, return 0.\n",
    "        \"\"\".format_map(Default(tweet_examples_of_non_category1=tweet_examples_of_non_category1, \\\n",
    "                               tweet_examples_of_category1=tweet_examples_of_category1))\n",
    "        \n",
    "        tweet_objects = Utility.get_tweet_data(input_file_name)\n",
    "        indices_to_ignore = category1_indices + non_category_1_indices\n",
    "        filtered_tweet_objects = tweet_objects.drop(indices_to_ignore)\n",
    "        \n",
    "        return system_message, filtered_tweet_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0bb48c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:42:53.156436Z",
     "start_time": "2023-11-06T17:42:53.140573Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_parameters = \"70b\"\n",
    "\n",
    "llm = TogetherLLM(\n",
    "    model= f\"togethercomputer/llama-2-{model_parameters}-chat\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=1500\n",
    ")\n",
    "\n",
    "input_file_name = \"tweets - original.csv\"\n",
    "output_file_name = \"updated_tweets_cat_1-{model_parameters}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bb0c7d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:42:53.188605Z",
     "start_time": "2023-11-06T17:42:53.158009Z"
    }
   },
   "outputs": [],
   "source": [
    "cat1 = Category1(llm, input_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1204b42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T17:56:23.270812Z",
     "start_time": "2023-11-06T17:42:53.497263Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cat1.generate_cat_metrics(output_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
