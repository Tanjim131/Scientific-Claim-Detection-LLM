{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf7bf83c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T09:10:56.607913Z",
     "start_time": "2023-11-08T09:10:52.234304Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import together\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "class TogetherLLM(LLM):\n",
    "    \"\"\"Together large language models.\"\"\"\n",
    "\n",
    "    model: str = \"togethercomputer/llama-2-13b-chat\"\n",
    "    \"\"\"model endpoint to use\"\"\"\n",
    "\n",
    "    together_api_key: str = os.environ['TOGETHERAI_API_KEY']\n",
    "    \"\"\"Together API key\"\"\"\n",
    "\n",
    "    temperature: float = 0.0\n",
    "    \"\"\"What sampling temperature to use.\"\"\"\n",
    "\n",
    "    max_tokens: int = 512\n",
    "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of LLM.\"\"\"\n",
    "        return \"together\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        **kwargs,\n",
    "    ) -> str:\n",
    "        \"\"\"Call to Together endpoint.\"\"\"\n",
    "        endpoint = 'https://api.together.xyz/inference'\n",
    "        \n",
    "        print(\"model =\", self.model)\n",
    "        print(\"temperature =\", self.temperature)\n",
    "        print(\"max_tokens=\", self.max_tokens)\n",
    "                \n",
    "        for attempt in range(10):\n",
    "            try:\n",
    "                res = requests.post(endpoint, json={\n",
    "                    \"prompt\": prompt,\n",
    "                    \"model\": self.model,\n",
    "                    \"temperature\": self.temperature,\n",
    "                    \"max_tokens\": self.max_tokens\n",
    "                }, headers={\n",
    "                    \"Authorization\": f\"Bearer {self.together_api_key}\",\n",
    "                    \"User-Agent\": \"<YOUR_APP_NAME>\"\n",
    "                })\n",
    "                output = res.json()['output']['choices'][0]['text']\n",
    "                return output\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        raise Exception(f\"Request did not succeed with prompt = {prompt}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d4a359e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T09:10:59.089818Z",
     "start_time": "2023-11-08T09:10:56.614282Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "class Utility:\n",
    "    B_CHAT, E_CHAT = \"<s>\", \"</s>\"\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_prompt(system_message, user_message, input_variables, history):\n",
    "        system_prompt = f\"{Utility.B_SYS}{system_message}{Utility.E_SYS}\"\n",
    "        \n",
    "        prompt_template_items = []\n",
    "                        \n",
    "        for index, (query, response) in enumerate(history):\n",
    "            if index == 0:\n",
    "                prompt_template_items.append(Utility.B_CHAT)\n",
    "                prompt_template_items.append(Utility.B_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(system_prompt)\n",
    "                prompt_template_items.append(query)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(Utility.E_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(response)\n",
    "                prompt_template_items.append(Utility.E_CHAT)\n",
    "            else:\n",
    "                prompt_template_items.append(Utility.B_CHAT)\n",
    "                prompt_template_items.append(Utility.B_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(query)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(Utility.E_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(response)\n",
    "                prompt_template_items.append(Utility.E_CHAT)\n",
    "        \n",
    "        if not history:\n",
    "            prompt_template_items.append(Utility.B_CHAT)\n",
    "            prompt_template_items.append(Utility.B_INST)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(system_prompt)\n",
    "            prompt_template_items.append(user_message)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(Utility.E_INST)\n",
    "        else:\n",
    "            prompt_template_items.append(Utility.B_CHAT)\n",
    "            prompt_template_items.append(Utility.B_INST)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(user_message)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(Utility.E_INST)\n",
    "        \n",
    "        prompt_template = \"\".join(prompt_template_items)\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=input_variables)\n",
    "        return prompt\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_type_from_response(response, search_term):\n",
    "        does_not_include_patterns = [\n",
    "            f\"\"\"that tweet as it is not {search_term.lower()}\"\"\",\n",
    "            f\"\"\"(the|this|the given) tweet.*is not {search_term.lower()}\"\"\",\n",
    "            f\"\"\"i.*classify (it|the tweet) as not {search_term.lower()}\"\"\",\n",
    "            f\"\"\"(?:the|this|the given)?\\s*(?:tweet|it) does not fall (into|under) the (?:category of )?{search_term.lower()}\"\"\",\n",
    "            f\"\"\"it is therefore classified as not {search_term.lower()}\"\"\",\n",
    "            f\"\"\"i would classify the (given )?tweet as not {search_term.lower()}\"\"\"\n",
    "        ]\n",
    "\n",
    "        includes_patterns = [\n",
    "            f\"\"\"(the|this|the given) tweet.*is {search_term.lower()}\"\"\",\n",
    "            f\"\"\"i.*classify (it|the tweet) as {search_term.lower()}\"\"\",\n",
    "            f\"\"\"(?:the|this|the given)?\\s*(?:tweet|it) (falls into|falls under|under) the (?:category of )?{search_term.lower()}\"\"\",\n",
    "            f\"\"\"this is a {search_term.lower()} claim as it is based on\"\"\",\n",
    "            f\"\"\"the tweet is classified as {search_term.lower()}\"\"\",\n",
    "            f\"\"\"this claim can be verified.*{search_term.lower()}\"\"\",\n",
    "            f\"\"\"therefore, it can be classified as {search_term.lower()}\"\"\",\n",
    "            f\"\"\"this is a direct statement about.*{search_term.lower()}\"\"\",\n",
    "            f\"\"\"so it falls under the category of {search_term.lower()}\"\"\",\n",
    "            f\"\"\"the tweet is reporting on a {search_term.lower()} fact\"\"\",\n",
    "            f\"\"\"this tweet contains a direct statement about.*{search_term.lower()}\"\"\"\n",
    "        ]\n",
    "\n",
    "        for pattern in does_not_include_patterns:\n",
    "            if re.search(pattern, response.lower()):\n",
    "                print(\"pattern does not include = \", pattern)\n",
    "                return '@'\n",
    "\n",
    "        for pattern in includes_patterns:\n",
    "            if re.search(pattern, response.lower()):\n",
    "                print(\"pattern include = \", pattern)\n",
    "                return '#'\n",
    "\n",
    "        for c in response[::-1]:\n",
    "            if c == '@' or c == '#':\n",
    "                print(\"Inside last search!\")\n",
    "                return c\n",
    "\n",
    "        raise Exception(f\"No @ or # found in response = {response}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_metrics(ground_truth, predicted):\n",
    "#         accuracy = accuracy_score(ground_truth, predicted)\n",
    "#         precision = precision_score(ground_truth, predicted)\n",
    "#         recall = recall_score(ground_truth, predicted)\n",
    "#         f1 = f1_score(ground_truth, predicted)\n",
    "        \n",
    "        clsf_report = classification_report(y_true = ground_truth, y_pred = predicted, output_dict=True)\n",
    "        cf_matrix = confusion_matrix(ground_truth, predicted)\n",
    "        \n",
    "        precision = clsf_report['weighted avg']['precision']\n",
    "        recall = clsf_report['weighted avg']['recall']\n",
    "        f1 = clsf_report['weighted avg']['f1-score']\n",
    "        accuracy = accuracy_score(ground_truth, predicted)\n",
    "        \n",
    "        return {\n",
    "            \"Accuracy\": accuracy * 100,\n",
    "            \"Precision\": precision * 100,\n",
    "            \"Recall\": recall * 100,\n",
    "            \"F1\": f1 * 100,\n",
    "            \"Confusion Matrix\": cf_matrix\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tweet_data(file_name):\n",
    "        df = pd.read_csv(file_name, index_col=0)\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_prediction_output(tweet_objects, file_name_to_write):\n",
    "        if os.path.exists(file_name_to_write):\n",
    "            os.remove(file_name_to_write)\n",
    "        \n",
    "        tweet_objects.to_csv(file_name_to_write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd491e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T09:10:59.103766Z",
     "start_time": "2023-11-08T09:10:59.089818Z"
    }
   },
   "outputs": [],
   "source": [
    "class Default(dict):\n",
    "    def __missing__(self, key):\n",
    "        return f\"{{{key}}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f39ff3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T10:13:06.881129Z",
     "start_time": "2023-11-08T10:13:06.852303Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "class Category:\n",
    "    INPUT_VARIABLES=[\"delimiter\", \"tweet\"]\n",
    "    \n",
    "    DELIMITER = \"```\"\n",
    "    \n",
    "    CATEGORY_DESCRIPTIONS = {1: \"Scientifically Verifiable\"}\n",
    "    \n",
    "    def __init__(self, category_type, llm):\n",
    "        self.category_type = category_type\n",
    "        \n",
    "        self.llm = llm\n",
    "        self.history = []\n",
    "        \n",
    "    def does_tweet_fall_into_category(self, tweet, search_term):\n",
    "        print(\"Inside does_tweet_fall_into_category function\")\n",
    "        \n",
    "        user_message = \"\"\"\n",
    "         Classify the following tweet:\n",
    "         Tweet: {delimiter} {tweet} {delimiter}\n",
    "         \"\"\"\n",
    "        \n",
    "        prompt = Utility.get_prompt(self.system_message, user_message, Category.INPUT_VARIABLES, [])\n",
    "\n",
    "        chain = LLMChain(llm = self.llm, prompt = prompt)\n",
    "\n",
    "        input_values = {\"tweet\": tweet, \"delimiter\": Category.DELIMITER}\n",
    "        \n",
    "        response = chain.run(input_values)\n",
    "        \n",
    "        print(\"Response =\", response)\n",
    "        \n",
    "        response_number = Utility.extract_type_from_response(response, search_term)\n",
    "        \n",
    "        return response_number\n",
    "        \n",
    "    def generate_cat_metrics(self, output_file_name, tweet_content_column=\"polished_text\"):\n",
    "        ground_truths = []\n",
    "        predicted_outputs = []\n",
    "\n",
    "        print(\"<======= Generating metrics for category type =\", self.category_type, \"=======>\")\n",
    "        print()\n",
    "\n",
    "        # check if predicted_cat_type column exists. If not, create it.\n",
    "\n",
    "        category_type_prediction_column_name = f\"predicted_{self.category_type}\"\n",
    "        \n",
    "        start_index = 201\n",
    "        end_index = 400\n",
    "\n",
    "        if category_type_prediction_column_name not in self.tweet_objects:\n",
    "            self.tweet_objects[category_type_prediction_column_name] = -1\n",
    "        else:\n",
    "            for index in range(start_index, end_index + 1):\n",
    "                if index not in self.tweet_objects[column_name]:\n",
    "                    continue\n",
    "\n",
    "                if self.tweet_objects[category_type_prediction_column_name][index] != -1:\n",
    "                    raise Exception(\n",
    "                        \"Some of the indices for the specified range have already been computed.\"\n",
    "                    )\n",
    "        \n",
    "#         was_previous_classification_correct = None\n",
    "        \n",
    "        for index in range(start_index, end_index + 1):\n",
    "            if index not in self.tweet_objects[tweet_content_column]:\n",
    "                continue\n",
    "                \n",
    "            print(\"Processing tweet with index# =\", index)\n",
    "            tweet = self.tweet_objects.iloc[index][tweet_content_column]\n",
    "            print(\"Tweet content = \", tweet)\n",
    "            \n",
    "            for attempt in range(10):\n",
    "                try:\n",
    "                    ground_truth = int(self.tweet_objects.iloc[index][f\"cat{self.category_type}\"])\n",
    "                except:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "                                \n",
    "            for attempt in range(10):\n",
    "                try:\n",
    "                    search_term = Category.CATEGORY_DESCRIPTIONS[self.category_type]\n",
    "                    predicted_output = self.does_tweet_fall_into_category(tweet, search_term)\n",
    "                    predicted_output = 0 if predicted_output == '@' else 1\n",
    "                    \n",
    "                    if predicted_output is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if predicted_output is None:\n",
    "                print(\"None for index# = \", index, \"and tweet content =\", tweet)\n",
    "                raise Exception(\"Did not get predicted output for tweet\")\n",
    "                \n",
    "            if index > 0 and index % 5 == 0:\n",
    "                print(\"Metrics till now =\", Utility.calculate_metrics(ground_truths, predicted_outputs))\n",
    "\n",
    "            ground_truths.append(ground_truth)\n",
    "            predicted_outputs.append(predicted_output)\n",
    "            \n",
    "            response_list = [\"Tweet is not scientifically verifiable\", \"Tweet is scientifically verifiable\"]\n",
    "            \n",
    "#             if len(self.history) == 20:\n",
    "#                 self.history.pop(0)\n",
    "            \n",
    "#             self.history.append((f\"Tweet = {tweet}\", response_list[predicted_output]))\n",
    "            \n",
    "#             if ground_truth == predicted_output:\n",
    "#                 was_previous_classification_correct = True\n",
    "#             else:\n",
    "#                 was_previous_classification_correct = False\n",
    "            \n",
    "            self.tweet_objects.loc[index, category_type_prediction_column_name] = predicted_output\n",
    "\n",
    "            print(\"Ground truth =\", ground_truth, \"Predicted output =\", predicted_output)\n",
    "            print(\"Finished Processing tweet with index# =\", index)\n",
    "            print()\n",
    "\n",
    "        print(\"<======= Finished generating metrics for claim existence =======>\")\n",
    "        \n",
    "        print(\"Ground truths = \", ground_truths)\n",
    "        print(\"Predictions = \", predicted_outputs)\n",
    "        \n",
    "        Utility.write_prediction_output(self.tweet_objects, output_file_name)\n",
    "        \n",
    "        return Utility.calculate_metrics(ground_truths, predicted_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13b2c52f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T10:13:07.471277Z",
     "start_time": "2023-11-08T10:13:07.451843Z"
    }
   },
   "outputs": [],
   "source": [
    "class Category1(Category):\n",
    "    CATEGORY_TYPE = 1\n",
    "    CATEGORY_DESCRIPTION = \"\"\n",
    "    \n",
    "    def __init__(self, llm, input_file_name):\n",
    "        self.system_message, self.tweet_objects = Category1.generate_system_prompt_for_category1(input_file_name)\n",
    "        \n",
    "        super().__init__(Category1.CATEGORY_TYPE, llm)\n",
    "        \n",
    "    @staticmethod\n",
    "    def generate_system_prompt_for_category1(input_file_name):\n",
    "        system_message = \"\"\"\n",
    "        Imagine you're a COVID-19 tweets classifier using the Clue And Reasoning Prompting (CARP) approach to discern scientifically verifiable claims from tweets.\n",
    "        \n",
    "        Tweets will be encased within {delimiter} characters. \n",
    "        \n",
    "        Apply the following steps:\n",
    "\n",
    "        1. CLUE IDENTIFICATION: Determine the presence of direct statements, reports, or factual claims related to COVID-19 using keywords and context within the tweet.\n",
    "\n",
    "        2. REASONING PROCESS: Analyze the clues to ascertain if they align with the scientific facts, data, or reputable health authority guidelines (Limit your reasoning to 130 words).\n",
    "\n",
    "        3. VERIFICATION DETERMINATION: Decide if the tweet's claim is scientifically verifiable, based on the evidence and reasoning.\n",
    "        \n",
    "        Example 1:\n",
    "        TWEET: \"New research indicates that COVID-19 can remain on surfaces for days.\"\n",
    "        CLUES: \"New research,\" \"COVID-19,\" \"remain on surfaces,\" \"days.\"\n",
    "        REASONING: The claim is presented as a finding from new research, which is a direct statement about the virus's transmission and is likely based on scientific studies.\n",
    "        VERIFICATION: # (Scientifically Verifiable)\n",
    "\n",
    "        Example 2:\n",
    "        TWEET: \"Most COVID-19 infections are mild and don't require hospitalization.\"\n",
    "        CLUES: \"Most,\" \"COVID-19 infections,\" \"mild,\" \"don't require,\" \"hospitalization.\"\n",
    "        REASONING: The statement makes a direct claim about the nature of COVID-19 infections, suggesting a general trend in symptoms and treatment requirements. This claim can be verified against statistical data from health authorities regarding the proportion of cases requiring hospitalization.\n",
    "        VERIFICATION: # (Scientifically Verifiable)\n",
    "\n",
    "        Example 3:\n",
    "        TWEET: \"Our city has reported zero new cases of COVID-19 today.\"\n",
    "        CLUES: \"Our city,\" \"reported,\" \"zero new cases,\" \"COVID-19,\" \"today.\"\n",
    "        REASONING: This is a direct report concerning COVID-19 cases, which can be verified with health department data and is a factual claim about the virus.\n",
    "        VERIFICATION: # (Scientifically Verifiable)\n",
    "\n",
    "        Example 4:\n",
    "        TWEET: \"How long does the virus stay airborne?\"\n",
    "        CLUES: \"How long,\" \"virus,\" \"stay airborne.\"\n",
    "        REASONING: This is a question rather than a claim, and it does not provide a direct statement or fact that can be scientifically verified.\n",
    "        VERIFICATION: @ (Not Scientifically Verifiable)\n",
    "        \n",
    "        Example 5:\n",
    "        TWEET: \"The government's response to COVID-19 will surely boost the economy.\"\n",
    "        CLUES: \"government's response,\" \"COVID-19,\" \"boost,\" \"economy.\"\n",
    "        REASONING: The input is making a speculative assertion about the impact of the government's COVID-19 response on the economy. While it relates to COVID-19, it is framed as a prediction rather than a fact and does not directly pertain to the scientific aspects of the virus itself. The claim is about economic impact, which is outside the scope of scientific verification as per the guidelines.\n",
    "        VERIFICATION: @ (Not Scientifically Verifiable)\n",
    "\n",
    "        Indicate your conclusion with a single symbol: use @ for non-verifiable claims or # for scientifically verifiable claims.\n",
    "        \"\"\"\n",
    "        \n",
    "        tweet_objects = Utility.get_tweet_data(input_file_name)\n",
    "        \n",
    "        return system_message, tweet_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0bb48c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T10:13:08.857799Z",
     "start_time": "2023-11-08T10:13:08.849957Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_parameters = \"70b\"\n",
    "\n",
    "llm = TogetherLLM(\n",
    "    model= f\"togethercomputer/llama-2-{model_parameters}-chat\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=1500\n",
    ")\n",
    "\n",
    "input_file_name = \"tweets - original.csv\"\n",
    "output_file_name = \"updated_tweets_cat_1-{model_parameters}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bb0c7d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T10:13:09.489352Z",
     "start_time": "2023-11-08T10:13:09.422901Z"
    }
   },
   "outputs": [],
   "source": [
    "cat1 = Category1(llm, input_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
