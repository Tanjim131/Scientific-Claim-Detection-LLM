{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4e566",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install transformers\n",
    "!pip install optimum\n",
    "!pip install auto-gptq\n",
    "!pip install --upgrade huggingface_hub\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de8566d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /s/chopin/b/grad/tanjim/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "access_token = \"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\"\n",
    "login(token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2beb4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "class Utility:\n",
    "    B_CHAT, E_CHAT = \"<s>\", \"</s>\"\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_prompt(system_message, user_message, input_variables, history):\n",
    "        system_prompt = f\"{Utility.B_SYS}{system_message}{Utility.E_SYS}\"\n",
    "        \n",
    "        prompt_template_items = []\n",
    "                        \n",
    "        for index, (query, response) in enumerate(history):\n",
    "            if index == 0:\n",
    "                prompt_template_items.append(Utility.B_CHAT)\n",
    "                prompt_template_items.append(Utility.B_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(system_prompt)\n",
    "                prompt_template_items.append(query)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(Utility.E_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(response)\n",
    "                prompt_template_items.append(Utility.E_CHAT)\n",
    "            else:\n",
    "                prompt_template_items.append(Utility.B_CHAT)\n",
    "                prompt_template_items.append(Utility.B_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(query)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(Utility.E_INST)\n",
    "                prompt_template_items.append(\" \")\n",
    "                prompt_template_items.append(response)\n",
    "                prompt_template_items.append(Utility.E_CHAT)\n",
    "        \n",
    "        if not history:\n",
    "            prompt_template_items.append(Utility.B_CHAT)\n",
    "            prompt_template_items.append(Utility.B_INST)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(system_prompt)\n",
    "            prompt_template_items.append(user_message)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(Utility.E_INST)\n",
    "        else:\n",
    "            prompt_template_items.append(Utility.B_CHAT)\n",
    "            prompt_template_items.append(Utility.B_INST)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(user_message)\n",
    "            prompt_template_items.append(\" \")\n",
    "            prompt_template_items.append(Utility.E_INST)\n",
    "        \n",
    "        prompt_template = \"\".join(prompt_template_items)\n",
    "        prompt = PromptTemplate(template=prompt_template, input_variables=input_variables)\n",
    "        return prompt\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_number(s):        \n",
    "#         start_index = s.find('{')\n",
    "#         end_index = s.find('}') + 1\n",
    "        \n",
    "#         substring = s[start_index : end_index]\n",
    "#         data = json.loads(substring)\n",
    "        \n",
    "#         key = list(data.keys())[0]\n",
    "#         value = data[key]\n",
    "        \n",
    "#         print(\"value = \", value)\n",
    "                \n",
    "#         if value == 0 or value == 1:\n",
    "#             return value\n",
    "        \n",
    "        for c in s[::-1]:\n",
    "            if c == '0' or c == '1':\n",
    "                return int(c)\n",
    "\n",
    "        raise Exception(f\"No 0 or 1 found in response = {s}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_metrics(ground_truth, predicted):\n",
    "#         accuracy = accuracy_score(ground_truth, predicted)\n",
    "#         precision = precision_score(ground_truth, predicted)\n",
    "#         recall = recall_score(ground_truth, predicted)\n",
    "#         f1 = f1_score(ground_truth, predicted)\n",
    "        \n",
    "        clsf_report = classification_report(y_true = ground_truth, y_pred = predicted, output_dict=True)\n",
    "        cf_matrix = confusion_matrix(ground_truth, predicted)\n",
    "        \n",
    "        precision = clsf_report['weighted avg']['precision']\n",
    "        recall = clsf_report['weighted avg']['recall']\n",
    "        f1 = clsf_report['weighted avg']['f1-score']\n",
    "        accuracy = accuracy_score(ground_truth, predicted)\n",
    "        \n",
    "        return {\n",
    "            \"Accuracy\": accuracy * 100,\n",
    "            \"Precision\": precision * 100,\n",
    "            \"Recall\": recall * 100,\n",
    "            \"F1\": f1 * 100,\n",
    "            \"Confusion Matrix\": cf_matrix\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_tweet_data(file_name):\n",
    "        df = pd.read_csv(file_name, index_col=0)\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_prediction_output(tweet_objects, file_name_to_write):\n",
    "        if os.path.exists(file_name_to_write):\n",
    "            os.remove(file_name_to_write)\n",
    "        \n",
    "        tweet_objects.to_csv(file_name_to_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aab32fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Default(dict):\n",
    "    def __missing__(self, key):\n",
    "        return f\"{{{key}}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eee669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "class Category:\n",
    "    INPUT_VARIABLES=[\"delimiter\", \"tweet\"]\n",
    "    \n",
    "    DELIMITER = \"```\"\n",
    "    \n",
    "    def __init__(self, category_type, llm):\n",
    "        self.category_type = category_type\n",
    "        \n",
    "        self.llm = llm\n",
    "        self.history = []\n",
    "        \n",
    "    def does_tweet_fall_into_category(self, tweet, was_previous_classification_correct=None):\n",
    "        print(\"Inside does_tweet_fall_into_category function\")\n",
    "                \n",
    "        if was_previous_classification_correct is None:\n",
    "            user_message = \"\"\"\n",
    "             Classify the following tweet:\n",
    "             Tweet: {delimiter} {tweet} {delimiter}\n",
    "             \"\"\"\n",
    "        else:\n",
    "            if was_previous_classification_correct:\n",
    "                user_message = \"\"\"\n",
    "                 Your previous classification was correct. Now, classify the following tweet:\n",
    "                 Tweet: {delimiter} {tweet} {delimiter}\n",
    "                 \"\"\"\n",
    "            else:\n",
    "                user_message = \"\"\"\n",
    "                 Your previous classification was incorrect. Now, classify the following tweet:\n",
    "                 Tweet: {delimiter} {tweet} {delimiter}\n",
    "                 \"\"\"\n",
    "                \n",
    "        # print(\"User message = \", user_message)\n",
    "        \n",
    "        verifiable_claim_chain_prompt = Utility.get_prompt(self.verifiable_claim_chain_system_message, \n",
    "                                                           user_message, Category.INPUT_VARIABLES, [])\n",
    "        \n",
    "#         print(\"verifiable_claim_chain_prompt = \", verifiable_claim_chain_prompt)\n",
    "#         print()\n",
    "#         print()\n",
    "        \n",
    "        non_verifiable_claim_chain_prompt = Utility.get_prompt(self.non_verifiable_claim_chain_system_message, \n",
    "                                                               user_message, Category.INPUT_VARIABLES, [])\n",
    "        \n",
    "#         print(\"non_verifiable_claim_chain_prompt = \", non_verifiable_claim_chain_prompt)\n",
    "#         print()\n",
    "#         print()\n",
    "\n",
    "        verifiable_claim_chain = LLMChain(llm = self.llm, \n",
    "                                          prompt = verifiable_claim_chain_prompt)\n",
    "        \n",
    "#         print(\"verifiable_claim_chain = \", verifiable_claim_chain)\n",
    "#         print()\n",
    "        \n",
    "        non_verifiable_claim_chain = LLMChain(llm = self.llm, \n",
    "                                          prompt = non_verifiable_claim_chain_prompt)\n",
    "        \n",
    "#         print(\"non_verifiable_claim_chain = \", non_verifiable_claim_chain)\n",
    "#         print()\n",
    "\n",
    "        input_values = {\"tweet\": tweet, \"delimiter\": Category.DELIMITER}\n",
    "        \n",
    "#         print(\"input values = \", input_values)\n",
    "        \n",
    "        verifiable_claim_chain_response = verifiable_claim_chain.run(input_values)\n",
    "        non_verifiable_claim_chain_response = non_verifiable_claim_chain.run(input_values)\n",
    "        \n",
    "#         print(\"verifiable_claim_chain_response = \", verifiable_claim_chain_response)\n",
    "#         print(\"non_verifiable_claim_chain_response = \", non_verifiable_claim_chain_response)\n",
    "        \n",
    "        verifiable_claim_chain_response_number = Utility.extract_number(verifiable_claim_chain_response)\n",
    "        non_verifiable_claim_chain_response_number = Utility.extract_number(non_verifiable_claim_chain_response)\n",
    "        \n",
    "#         print(\"verifiable_claim_chain_response_number = \", verifiable_claim_chain_response_number)\n",
    "#         print(\"non_verifiable_claim_chain_response_number = \", non_verifiable_claim_chain_response_number)\n",
    "        \n",
    "        if verifiable_claim_chain_response_number != non_verifiable_claim_chain_response_number:\n",
    "            return verifiable_claim_chain_response_number\n",
    "        \n",
    "        arbitrer_claim_chain_user_message = \"\"\"\n",
    "        Response1: {response1}\n",
    "        Response2: {response2}\n",
    "        \"\"\"\n",
    "        \n",
    "        arbitrer_claim_chain_input_variables = [\"response1\", \"response2\"]\n",
    "        \n",
    "        arbitrer_claim_chain_prompt = Utility.get_prompt(self.arbitrer_claim_chain_system_message, \n",
    "                                                         arbitrer_claim_chain_user_message, \n",
    "                                                         arbitrer_claim_chain_input_variables,\n",
    "                                                         self.history)\n",
    "        \n",
    "        arbitrer_claim_input_values = {\"response1\": verifiable_claim_chain_response, \n",
    "                                       \"response2\": non_verifiable_claim_chain_response}\n",
    "        \n",
    "                \n",
    "        arbitrer_claim_chain = LLMChain(llm = self.llm, \n",
    "                                        prompt = arbitrer_claim_chain_prompt)\n",
    "        \n",
    "        arbitrer_response = arbitrer_claim_chain.run(arbitrer_claim_input_values)\n",
    "        \n",
    "        return Utility.extract_number(arbitrer_response)\n",
    "        \n",
    "    def generate_cat_metrics(self, output_file_name, tweet_content_column=\"polished_text\"):\n",
    "        ground_truths = []\n",
    "        predicted_outputs = []\n",
    "\n",
    "        print(\"<======= Generating metrics for category type =\", self.category_type, \"=======>\")\n",
    "        print()\n",
    "\n",
    "        # check if predicted_cat_type column exists. If not, create it.\n",
    "\n",
    "        category_type_prediction_column_name = f\"predicted_{self.category_type}\"\n",
    "        \n",
    "        start_index = 0\n",
    "        end_index = 200\n",
    "\n",
    "        if category_type_prediction_column_name not in self.tweet_objects:\n",
    "            self.tweet_objects[category_type_prediction_column_name] = -1\n",
    "        else:\n",
    "            for index in range(start_index, end_index + 1):\n",
    "                if index not in self.tweet_objects[column_name]:\n",
    "                    continue\n",
    "\n",
    "                if self.tweet_objects[category_type_prediction_column_name][index] != -1:\n",
    "                    raise Exception(\n",
    "                        \"Some of the indices for the specified range have already been computed.\"\n",
    "                    )\n",
    "        \n",
    "        was_previous_classification_correct = None\n",
    "        \n",
    "        for index in range(start_index, end_index + 1):\n",
    "            if index not in self.tweet_objects[tweet_content_column]:\n",
    "                continue\n",
    "                \n",
    "            print(\"Processing tweet with index# =\", index)\n",
    "            tweet = self.tweet_objects.iloc[index][tweet_content_column]\n",
    "            print(\"Tweet content = \", tweet)\n",
    "            \n",
    "            for attempt in range(10):\n",
    "                try:\n",
    "                    ground_truth = int(self.tweet_objects.iloc[index][f\"cat{self.category_type}\"])\n",
    "                except:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "                                \n",
    "            for attempt in range(10):\n",
    "                try:\n",
    "                    predicted_output = self.does_tweet_fall_into_category(tweet,\n",
    "                                                                          was_previous_classification_correct)\n",
    "                    \n",
    "                    if predicted_output is None:\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if predicted_output is None:\n",
    "                print(\"None for index# = \", index, \"and tweet content =\", tweet)\n",
    "                raise Exception(\"Did not get predicted output for tweet\")\n",
    "                \n",
    "            if index > 0 and index % 5 == 0:\n",
    "                print(\"Metrics till now =\", Utility.calculate_metrics(ground_truths, predicted_outputs))\n",
    "\n",
    "            ground_truths.append(ground_truth)\n",
    "            predicted_outputs.append(predicted_output)\n",
    "            \n",
    "            response_list = [\"Tweet is not scientifically verifiable\", \"Tweet is scientifically verifiable\"]\n",
    "            \n",
    "#             if len(self.history) == 20:\n",
    "#                 self.history.pop(0)\n",
    "            \n",
    "#             self.history.append((f\"Tweet = {tweet}\", response_list[predicted_output]))\n",
    "            \n",
    "            if ground_truth == predicted_output:\n",
    "                was_previous_classification_correct = True\n",
    "            else:\n",
    "                was_previous_classification_correct = False\n",
    "            \n",
    "            self.tweet_objects.loc[index, category_type_prediction_column_name] = predicted_output\n",
    "\n",
    "            print(\"Ground truth =\", ground_truth, \"Predicted output =\", predicted_output)\n",
    "            print(\"Finished Processing tweet with index# =\", index)\n",
    "            print()\n",
    "\n",
    "        print(\"<======= Finished generating metrics for claim existence =======>\")\n",
    "        \n",
    "        print(\"Ground truths = \", ground_truths)\n",
    "        print(\"Predictions = \", predicted_outputs)\n",
    "        \n",
    "        Utility.write_prediction_output(self.tweet_objects, output_file_name)\n",
    "        \n",
    "        return Utility.calculate_metrics(ground_truths, predicted_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f0dc4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Category1(Category):\n",
    "    CATEGORY_TYPE = 1\n",
    "    CATEGORY_DESCRIPTION = \"\"\n",
    "    \n",
    "    def __init__(self, llm, input_file_name):\n",
    "        self.verifiable_claim_chain_system_message, \\\n",
    "        self.non_verifiable_claim_chain_system_message, \\\n",
    "        self.arbitrer_claim_chain_system_message, \\\n",
    "        self.tweet_objects = Category1.generate_system_prompt_for_category1(input_file_name)\n",
    "        \n",
    "        super().__init__(Category1.CATEGORY_TYPE, llm)\n",
    "        \n",
    "    @staticmethod\n",
    "    def generate_system_prompt_for_category1(input_file_name):\n",
    "        category1_indices = [22, 56, 71]\n",
    "        non_category_1_indices = [21, 61, 90]\n",
    "\n",
    "        tweet_examples_of_category1 = \"\"\"\n",
    "        Some examples of tweets that ARE scientifically verifiable (expected response 1):\n",
    "            a) \" ::people_holding_hands:: We can now meet our family and friends outdoors in a group of 6, or 2 households ::leftright_arrow:: Its important that when we do, we follow social distancing guidance ::backhand_index_pointing_right:: This will help to stop the spread of COVID19 as we take the next step out of lockdown LetsDoItForLancashire \"\n",
    "            b) \": BREAKING: Dozens of cops in Massachusetts have resigned in protest of the vaccine mandates. TO WISH THEM GOOD RIDDA\"\n",
    "            c) \": BREAKING Syria president and first lady test positive for COVID19: presidency AFP\"\n",
    "        \"\"\"\n",
    "        \n",
    "        tweet_examples_of_non_category1 = \"\"\"\n",
    "        Some examples of tweets ARE NOT scientifically verifiable (expected response 0):\n",
    "            a) \" : The ones calling for lockdown, without risk or injury to themselves, should pay up.\"\n",
    "            b) \": Can you catch coronavirus from handling cash? A new study says the risk is low\"\n",
    "            c) \": I wouldnt trust anything this man touches. NoVaccineForMe\"\n",
    "        \"\"\"\n",
    "        \n",
    "        verifiable_claim_chain_system_message = \"\"\"\n",
    "        Imagine you're a COVID-19 tweets classifier. You will determine whether tweets fall into scientifically verifiable claim category using the following guidelines:\n",
    "\n",
    "            I) Direct statements about the COVID-19 virus, its origin, its transmission, prevention methods, or symptoms etc ARE scientifically verifiable. For example:\n",
    "                - Example 1: \"Masks don't work against COVID-19.\"\n",
    "                - Example 2: \"The government needs to get to the bottom of COVID-19 origin and Chinese involvement.\"\n",
    "\n",
    "            II) Opinionated, anecdotal, or hearsay claims about COVID-19 topics MAY BE scientifically verifiable:\n",
    "                - Example 1: \"Talked to a friend who believes the virus started from bats in a wet market. Sounds plausible.\" (Hearsay)\n",
    "                - Example 2: \"Got my vaccine yesterday and I feel great! Proof that it works\" (Anecdote)\n",
    "                - Example 3: \"Based on my research, I'm convinced the virus started from bats in a wet market.\" (Opinion)\n",
    "                - Example 4: \"Don't forget to practice social distancing. It will keep everyone safe.\" (Opinion)\n",
    "\n",
    "            III) Reports on cases, deaths, or someone testing positive ARE scientifically verifiable. For example:\n",
    "                - Example 1: \"Justin Bieber has tested positive for COVID19\"\n",
    "                - Example 3: \"Almost 2000 people have died from COVID-19 in Brazil\"\n",
    "\n",
    "        For a given tweet, step through each of the points. If there's a match with one of the points, return 1. If there's no match with any of the points, return 0. Your response should contain the entire process of stepping through the guidelines.\n",
    "\n",
    "        Output Instructions:\n",
    "                - Tweet = \"Yall can try that damn vaccine on yourselves first! Im not trying to turn into an anamorph\"\n",
    "                    - point I) doesn't match as this is not a direct statement about COVID-19 topics.\n",
    "                    - point II) matches as the tweet contains an opinion about the vaccine (a COVID-19 topic)\n",
    "                        - The tweet is scientifically verifiable. Return 1.\n",
    "\n",
    "                - Tweet = \"Yo jus dont use Amazon this lockdown, well were all gettin burnt 2 the ground, bezos just rackin n not payin tax to help the nhs, schools, carework, nothing.. if u feed the beast yr just helpin them destroy us..\"\n",
    "                    - point I) doesn't match as there's no direct statement about COVID-19 topics\n",
    "                    - point II) doesn't match as there's no opinions about COVID-19 topics\n",
    "                    - point III) doesn't match as there's no report\n",
    "                    - The tweet is not scientifically verifiable. Return 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        non_verifiable_claim_chain_system_message = \"\"\"\n",
    "        Imagine you're a COVID-19 tweets classifier. You will determine whether tweets do not fall into scientifically verifiable claim category using the following guidelines:\n",
    "\n",
    "            I) Observational statements ARE NOT scientifically verifiable. For example:\n",
    "                - Example 1: \"Many people aren't wearing masks\"\n",
    "                - Example 2: \"People aren't really following social distancing, apparently\"\n",
    "\n",
    "            II) Impacts of COVID-19 on fields other than science - Business, Law, Histoy, Politics, Operations etc ARE NOT scientifically verifiable. For example:\n",
    "                - Example 1: \"We haven't been able to open our restaurant as COVID-19 has impacted us to operate at full capacity.\" (COVID-19's effect on Business)\n",
    "                - Example 2: \"COVID-19 disrupted global supply chains, leading to shortages of essential goods and a rise in production costs.\" (COVID-19's effect on Operations)\n",
    "                - Example 3: \"Legal disputes over lease agreements surged during the pandemic, particularly where tenants were unable to meet their rental obligations due to lockdowns\" (COVID-19's effect on Law)\n",
    "                - Example 4: \"The pandemic triggered geopolitical tensions, with countries competing for access to limited vaccine supplies and engaging in 'vaccine diplomacy'.\" (COVID-19's effect on Politics)\n",
    "                - Example 5: \"Film and television production faced long hiatuses, and when resumed, had to adapt to strict health protocols.\" (COVID-19's effect on Entertainment)\n",
    "\n",
    "            III) Second-hand opinions or queries ARE NOT scientifically verifiable. For example:\n",
    "                - Example 1: \"My neighbor says that social distancing is just a way to keep us apart and isolated. Thoughts?\" (The opinion is not author's, but his neighbor's)\n",
    "                - Example 2: \"They have said we need more social distancing even after vaccines. I don't understand why.\" (The first part is not author's saying.)\n",
    "\n",
    "            IV) Asking questions without a direct claim ARE NOT scientifically verifiable. For example:\n",
    "                - Example 1: \"Why do we still need to wear masks after vaccination?\"\n",
    "                - Example 2: \"I'm surprised. Weren't they all vaccinated at the company conference?\"\n",
    "\n",
    "            V) Instructions, Information, notifications or announcements that do not contain opinions about COVID-19 topics ARE NOT scientifically verifiable. For examples:\n",
    "                - Example 1: \"You need to wear masks and follow social distancing to get on buses, trains or planes\" (Instructions)\n",
    "                - Example 2: \"Due to social distancing, our restaurant hasn't been able to operate at full capacity\" (Dispatching Information)\n",
    "                - Example 3: \"Travel advisory: If you're returning from a hotspot, you need to self-quarantine for 14 days.\" (Announcement)\n",
    "                - Example 4: \"You will receive communication if you are eligible for the vaccine.\" (Notification)\n",
    "                - Example 5: \"Get your free COVID-19 test by just walking in a clinic today\" (Announcement)\n",
    "                - Example 6: \"A hospital is using a new software to track COVID-19 cases.\"\n",
    "\n",
    "            VI) Political, Business or Legal motive behind COVID-19 topics ARE NOT scientifically verifiable. For example:\n",
    "                - Example 1: \"The Trump administration could have sped up the vaccine development process. If it was a democratic president, they would have dont it.\"\n",
    "                - Example 2: \"The pharmaceutical companies were not pressured politically by the government to deliver vaccines.\"\n",
    "\n",
    "            VII) Phrases like \"Read the whole story here\", \"Full version\", \"This story from\", \"Live Video\", \"How it became\", \"Here's a quick look\" etc. means it is a news reporting. These tweets DO NOT contain scientifically verifiable claim.\n",
    "\n",
    "        For a given tweet, step through each of the points. If there's a match with one of the points, return 1. If there's no match with any of the points, return 0. Your response should contain the entire process of stepping through the guidelines.\n",
    "\n",
    "        Output Instructions:\n",
    "        - Tweet = \"Yall can try that damn vaccine on yourselves first! Im not trying to turn into an anamorph\"\n",
    "            - point I) doesn't match as this is not an observation.\n",
    "            - point II) doesn't match as the tweet doesn't mention impact of non-scientific fields\n",
    "            - point III) doesn't match as the tweet doesn't contain second-hand opinions\n",
    "            - point IV) doesn't match as the tweet doesn't pose a question\n",
    "            - point V) doesn't match as the tweet doesn't contain instructions, info, notification or annoucements that do not contain opinion\n",
    "            - point VI) doesn't match as there's no motive\n",
    "            - point VII) doesn't match as there's no such phrase\n",
    "            - The tweet is scientifically verifiable. Return 0.\n",
    "\n",
    "        - Tweet = \"Yo jus dont use Amazon this lockdown, well were all gettin burnt 2 the ground, bezos just rackin n not payin tax to help the nhs, schools, carework, nothing.. if u feed the beast yr just helpin them destroy us..\"\n",
    "            - point I) doesn't match as this is not an observation.\n",
    "            - point II) matches as the tweet mentions the impact of Amazon's predatory business policies on everyday life.\n",
    "            - The tweet is not scientifically verifiable. Return 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        arbitrer_claim_chain_system_message = \"\"\"\n",
    "        Imagine you're a COVID-19 tweets classifier. You will receive two responses for each tweet - one being the argument that the tweet is scientifically verifiable, and other that the tweet is not scientifically verifiable. Your job is to decide which one is correct.\n",
    "\n",
    "        Give your reasoning which one of the two responses is more accurate using the following guidelines:\n",
    "            I) Direct statements about the COVID-19 virus, its origin, its transmission, prevention methods, or symptoms etc ARE scientifically verifiable.\n",
    "            II) Opinionated, anecdotal, or hearsay claims about COVID-19 topics MAY BE scientifically verifiable.\n",
    "            III) Reports on cases, deaths, or someone testing positive ARE scientifically verifiable.\n",
    "            IV) Observational statements ARE NOT scientifically verifiable. Focus on the difference between observations and opinions. Opinions have claims. Observations do not have claims.\n",
    "            V) Impacts of COVID-19 on fields other than science - Business, Law, Histoy, Politics, Operations etc ARE NOT scientifically verifiable.\n",
    "            VI) Second-hand opinions or queries ARE NOT scientifically verifiable.\n",
    "            VII) Asking questions without a direct claim ARE NOT scientifically verifiable.\n",
    "            VIII) Instructions, Information, notifications or announcements that do not contain opinions about COVID-19 topics ARE NOT scientifically verifiable.\n",
    "            IX) Political, Business or Legal motive behind COVID-19 topics ARE NOT scientifically verifiable.\n",
    "            X) Phrases like \"Read the whole story here\", \"Full version\", \"This story from\", \"Live Video\", \"How it became\", \"Here's a quick look\" etc. means it is a news reporting. These tweets DO NOT contain scientifically verifiable claim.\n",
    "            \n",
    "        {tweet_examples_of_category1}\n",
    "        \n",
    "        {tweet_examples_of_non_category1}\n",
    "\n",
    "        If the tweet is scientifically verifiable, return 1. Otherwise, return 0.\n",
    "        \"\"\".format_map(Default(tweet_examples_of_non_category1=tweet_examples_of_non_category1, \\\n",
    "                               tweet_examples_of_category1=tweet_examples_of_category1))\n",
    "        \n",
    "        tweet_objects = Utility.get_tweet_data(input_file_name)\n",
    "        indices_to_ignore = category1_indices + non_category_1_indices\n",
    "        filtered_tweet_objects = tweet_objects.drop(indices_to_ignore)\n",
    "        \n",
    "        return verifiable_claim_chain_system_message, \\\n",
    "                non_verifiable_claim_chain_system_message, \\\n",
    "                arbitrer_claim_chain_system_message, \\\n",
    "                filtered_tweet_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "631c1c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, logging\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "class HuggingFace:\n",
    "    def __init__(self, model_id, temperature, max_new_tokens):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    \n",
    "        model = AutoGPTQForCausalLM.from_quantized(model_id,\n",
    "            model_basename=\"model\",\n",
    "            inject_fused_attention=False, # Required for Llama 2 70B model at this time.\n",
    "            use_safetensors=True,\n",
    "            trust_remote_code=False,\n",
    "            device=\"cuda:0\",\n",
    "            quantize_config=None)\n",
    "\n",
    "        # Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
    "        logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.15\n",
    "        )\n",
    "\n",
    "        self.llm = HuggingFacePipeline(pipeline = pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "604227cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "hf = HuggingFace(\n",
    "    model_id=\"TheBloke/Llama-2-70B-Chat-GPTQ\",\n",
    "    temperature=0.15,\n",
    "    max_new_tokens=2048\n",
    ")\n",
    "\n",
    "llm = hf.llm\n",
    "input_file_name = \"tweets - original.csv\"\n",
    "output_file_name = \"updated_tweets_cat_1-70b.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01dba137",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cat1 = Category1(llm, input_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d618f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHuggingFacePipeline\u001b[0m\n",
      "Params: {'model_id': 'gpt2', 'model_kwargs': None, 'pipeline_kwargs': None}\n"
     ]
    }
   ],
   "source": [
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b4def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<======= Generating metrics for category type = 1 =======>\n",
      "\n",
      "Processing tweet with index# = 0\n",
      "Tweet content =           Hi , We have passed your comment onto our COVID 19 Social Distancing Team. Many Thanks Steve\n",
      "Inside does_tweet_fall_into_category function\n",
      "Ground truth = 0 Predicted output = 1\n",
      "Finished Processing tweet with index# = 0\n",
      "\n",
      "Processing tweet with index# = 1\n",
      "Tweet content =       Section 51ix gives the commonwealth the power to make laws with respect to quarantine such as the Biosecurity Act but doesnt make quarantine an exclusive federal responsibility. Cth law can override state law, but only insofar as they are incompatible.\n",
      "Inside does_tweet_fall_into_category function\n",
      "Ground truth = 0 Predicted output = 0\n",
      "Finished Processing tweet with index# = 1\n",
      "\n",
      "Processing tweet with index# = 2\n",
      "Tweet content =     All vaccine passports will allow you to do is to collapse social distancing. Given not everyone is immune even with a vaccine, and some cant or dont want to have it, you end up with a situation where the virus can run rampant. Remember we dont know it stops transmission\n",
      "Inside does_tweet_fall_into_category function\n",
      "Ground truth = 1 Predicted output = 1\n",
      "Finished Processing tweet with index# = 2\n",
      "\n",
      "Processing tweet with index# = 3\n",
      "Tweet content =     Apparently, workers on the set were standing too close together and not social distancing and following COVID safety protocols.\n",
      "Inside does_tweet_fall_into_category function\n",
      "Ground truth = 0 Predicted output = 1\n",
      "Finished Processing tweet with index# = 3\n",
      "\n",
      "Processing tweet with index# = 4\n",
      "Tweet content =     GOP   From what Im told by someone married to a recently hired Biden staff member, China and getting to the bottom of the COVID origin and why is very much a priority. Unlike the dipshit though, Biden will try diplomacy first and Im sure have spies on the ground.\n",
      "Inside does_tweet_fall_into_category function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/b/grad/tanjim/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth = 1 Predicted output = 0\n",
      "Finished Processing tweet with index# = 4\n",
      "\n",
      "Processing tweet with index# = 5\n",
      "Tweet content =     Hi Kevin, due to the Covid19 pandemic, parcel volumes have peaked beyond Christmas volumes and for the safety of our staff in our mail centres amp; those delivering the mail, our operational procedures have increasingly slowed due to social distancing protocol set out by the HSE.\n",
      "Inside does_tweet_fall_into_category function\n",
      "Metrics till now = {'Accuracy': 40.0, 'Precision': 43.33333333333333, 'Recall': 40.0, 'F1': 40.0, 'Confusion Matrix': array([[1, 2],\n",
      "       [1, 1]])}\n",
      "Ground truth = 0 Predicted output = 1\n",
      "Finished Processing tweet with index# = 5\n",
      "\n",
      "Processing tweet with index# = 6\n",
      "Tweet content =    Damn. IDK where youre located but my citys hotline was helpful in directing me toward the county, based on supply amp; demand. Someone also Tweeted about walkin spots in in south NJ amp; I know from my sis distributing the vaccine in NC contd\n",
      "Inside does_tweet_fall_into_category function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/b/grad/tanjim/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth = 0 Predicted output = 0\n",
      "Finished Processing tweet with index# = 6\n",
      "\n",
      "Processing tweet with index# = 7\n",
      "Tweet content =    Hello, masks and social distancing are required in order to use public transit. We have been using more high capacity amp; extra buses on routes, and cleaning our buses with hospitalgrade disinfectant, to mention a few. Please visit \n",
      "Inside does_tweet_fall_into_category function\n",
      "Ground truth = 0 Predicted output = 1\n",
      "Finished Processing tweet with index# = 7\n",
      "\n",
      "Processing tweet with index# = 8\n",
      "Tweet content =    If youre saying there have probably been ten enjoyable games in 2020, thats probably underlining my point. Theyve played 42. And I dont think youll find there were many between Mar and Dec 2019. Period just either side of lockdown was good. PSG amp; Leipzig recently excellent\n",
      "Inside does_tweet_fall_into_category function\n",
      "Ground truth = 0 Predicted output = 1\n",
      "Finished Processing tweet with index# = 8\n",
      "\n",
      "Processing tweet with index# = 9\n",
      "Tweet content =    in Turkey, there is going to be a lockdown from 31st december evening until january 4, morning. So if you want to party you have to make it a 4day bender. game on Turkey! Who will take the challenge? \n",
      "Inside does_tweet_fall_into_category function\n",
      "Inside does_tweet_fall_into_category function\n",
      "Ground truth = 0 Predicted output = 1\n",
      "Finished Processing tweet with index# = 9\n",
      "\n",
      "Processing tweet with index# = 10\n",
      "Tweet content =    It is helpful, but antibodies are not good indicators of immunity. Antibody titers fluctuate constantly regardless of infection or immunity. You want a strong Tcell response for immunity against a virus. Same goes for a vaccine. You want one the elicits a strong Tcell response\n",
      "Inside does_tweet_fall_into_category function\n",
      "Metrics till now = {'Accuracy': 30.0, 'Precision': 56.19047619047619, 'Recall': 30.0, 'F1': 33.53535353535354, 'Confusion Matrix': array([[2, 6],\n",
      "       [1, 1]])}\n",
      "Ground truth = 1 Predicted output = 0\n",
      "Finished Processing tweet with index# = 10\n",
      "\n",
      "Processing tweet with index# = 11\n",
      "Tweet content =    Pfizer wasnt pressured politically to make a vaccine like the way Operation Warp Speed was. See what happens when agencies are not used for political, personal gain? They get things done.\n",
      "Inside does_tweet_fall_into_category function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/b/grad/tanjim/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth = 0 Predicted output = 1\n",
      "Finished Processing tweet with index# = 11\n",
      "\n",
      "Processing tweet with index# = 12\n",
      "Tweet content =    The masks arent 100 and some people do not wear them correctly nor do they have filters, this virus is TOUGH so a flimsy cheap face masks isnt as good as one with a filter gt; \n",
      "Inside does_tweet_fall_into_category function\n",
      "Ground truth = 0 Predicted output = 1\n",
      "Finished Processing tweet with index# = 12\n",
      "\n",
      "Processing tweet with index# = 13\n",
      "Tweet content =    Yet they have already said that after the vaccine, you still have restrictions like mask wearing and social distancing. Interesting.\n",
      "Inside does_tweet_fall_into_category function\n",
      "Inside does_tweet_fall_into_category function\n",
      "Ground truth = 1 Predicted output = 0\n",
      "Finished Processing tweet with index# = 13\n",
      "\n",
      "Processing tweet with index# = 14\n",
      "Tweet content =   : ::loudspeaker:: Face masks and gathering restrictions are easing in parts of Queensland from 4pm today, Friday 8 October. Full detail\n",
      "Inside does_tweet_fall_into_category function\n",
      "Ground truth = 0 Predicted output = 1\n",
      "Finished Processing tweet with index# = 14\n",
      "\n",
      "Processing tweet with index# = 15\n",
      "Tweet content =   : all of us, when quarantine is over\n",
      "Inside does_tweet_fall_into_category function\n",
      "Metrics till now = {'Accuracy': 20.0, 'Precision': 32.00000000000001, 'Recall': 20.0, 'F1': 22.142857142857146, 'Confusion Matrix': array([[2, 9],\n",
      "       [3, 1]])}\n",
      "Ground truth = 0 Predicted output = 1\n",
      "Finished Processing tweet with index# = 15\n",
      "\n",
      "Processing tweet with index# = 16\n",
      "Tweet content =   : GulkiJoshi tests coronavirus positive, shares glimpses of her quarantine days with fans \n",
      "Inside does_tweet_fall_into_category function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/b/grad/tanjim/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth = 0 Predicted output = 1\n",
      "Finished Processing tweet with index# = 16\n",
      "\n",
      "Processing tweet with index# = 17\n",
      "Tweet content =   : How could this be, were they not using the mask vaccine on the Harris campaign?\n",
      "Inside does_tweet_fall_into_category function\n",
      "Ground truth = 1 Predicted output = 1\n",
      "Finished Processing tweet with index# = 17\n",
      "\n",
      "Processing tweet with index# = 18\n",
      "Tweet content =   : I love being Jewish. We get 8 nights of presents, AND we all got the vaccine in January 2015\n",
      "Inside does_tweet_fall_into_category function\n",
      "Ground truth = 0 Predicted output = 1\n",
      "Finished Processing tweet with index# = 18\n",
      "\n",
      "Processing tweet with index# = 19\n",
      "Tweet content =   : New York City all began with Little Italy. once the vaccine drops, check out where it all began!\n",
      "Inside does_tweet_fall_into_category function\n",
      "Ground truth = 0 Predicted output = 1\n",
      "Finished Processing tweet with index# = 19\n",
      "\n",
      "Processing tweet with index# = 20\n",
      "Tweet content =   : Nursing home gains threatened by lack of vaccine, unvaccinated workers. Story from  \n",
      "Inside does_tweet_fall_into_category function\n",
      "Metrics till now = {'Accuracy': 20.0, 'Precision': 33.333333333333336, 'Recall': 20.0, 'F1': 20.0, 'Confusion Matrix': array([[ 2, 13],\n",
      "       [ 3,  2]])}\n",
      "Ground truth = 0 Predicted output = 1\n",
      "Finished Processing tweet with index# = 20\n",
      "\n",
      "Processing tweet with index# = 23\n",
      "Tweet content =   24 be able to enter Canada, as entry to the country is at the discretion of a border services officer. Sorry we cant be more specific. Learn more about new restrictions for travel to Canada and additional testing and quarantine requirements\n",
      "Inside does_tweet_fall_into_category function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/b/grad/tanjim/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside does_tweet_fall_into_category function\n"
     ]
    }
   ],
   "source": [
    "cat1.generate_cat_metrics(output_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
